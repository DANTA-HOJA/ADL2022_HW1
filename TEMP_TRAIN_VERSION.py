    # # TODO: Training loop - iterate over train dataloader and update model weights

    # # DO: add some parameters
    # train_currbatch_loss = 0 # loss of current batch
    # train_cum_loss = 0 # an accumulation of loss of all batches
    # num_train_batch = 0 # how many batches are generated by dataloader with train_set
    # train_avg_loss = 0 # train_avg_loss = train_cum_loss/num_train_batch

    # model.train()
    # batch_bar = tqdm(dataloaders["train"], desc="Train")
    # for batch_index, data in enumerate(batch_bar): # Each time, take [# = batch_size] of data
        
    #     # DO: unpackage the data and sent to CPU/GPU
    #     sentence, intent = data # sentence.shape = [batch_size, sentence_max_len]
    #                             # intent.shape = [batch_size, 1]
    #     sentence, intent = sentence.to(device), intent.to(device)
    #     tqdm.write(f"sentence.shape = {sentence.shape}, intent.shape = {intent.shape}")
    #     # CHECK_PT: 
    #     input("\n=> press Enter to continue")
        
    #     # DO: train with this batch, and get prediction
    #     pred = model(sentence) # return softmax(IntentCls_RNN) or logsoftmax(IntentCls_LSTM) for each class
    #                            # pred.shape = [batch_size, intent_classes]
    #     loss = loss_fn(pred, intent) # loss_fn(para:softmax_prob, para:intent_in_index)
    #     # CHECK_PT:
    #     input("\n=> press Enter to continue")
        
    #     # DO: update the gradient using backpropagation
    #     optimizer.zero_grad() # zero the parameter of gradients before backpropagation
    #     loss.backward() # use loss to do backpropagation
    #     optimizer.step() # update the optimizer with gradient


    # # TODO: Evaluation loop - calculate accuracy and save model weights
        
        
        